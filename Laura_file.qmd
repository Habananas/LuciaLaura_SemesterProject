---
title: "Strava_File"
format: html
---

# STRAVA PROJECT 

### Libraries and Files
```{r}
# install.packages("XML")
# install.packages("gitcreds")
library("readr")
library(sf) #simple features 
library(ggplot2)
library(dplyr)
library("gitcreds")
library(XML) #to read the XML data of gpx files
library(tmap) #to show data in map viewer
#library(leaflet) #alternative to show in a map
library(lubridate) # time
library(knitr) #To “prove” that script runs on your machine from top to bottom
#install.packages("slider")
library(slider) # for "sliding" over the datapoints (similar to leadlag or roll)
#install.packages("factoextra")
#install.packages("cluster")
library(factoextra)#kmeans
library(cluster)#kmeans
library(zoo) # for sinuosity
```



### Data Load and Organisation
```{r}
laura_act <- read.csv("data/activities_Laura.csv")

# Get a list of files in the folder
folder_path <- "data/activities_Laura/"
file_list <- list.files(folder_path, full.names = TRUE)

#Create a function that assigns coordinates, elevation, time and activity name out of the gpx file, then apply this function to all of the gpx files:
gpx_to_df <- function(gpx_path) {
  gpx_parsed <- htmlTreeParse(file = gpx_path, useInternalNodes = TRUE)
  # read out elements of the html file to vecotrs
coords <- xpathSApply(doc = gpx_parsed, path = "//trkpt", fun = xmlAttrs)
elevation <- xpathSApply(doc = gpx_parsed, path = "//trkpt/ele", fun = xmlValue)
time <- xpathSApply(doc = gpx_parsed, path = "//time", fun = xmlValue)
activity_name <- xpathSApply(doc = gpx_parsed, path = "//name", fun = xmlValue)
activity_type <- xpathSApply(doc = gpx_parsed, path = "//type", fun = xmlValue)
# remove first value of time, as it stems from the metadata and matches the second value (i.e. first timestamp of trackpoint)
time <- time[-1]
# convert vectors to a data frame
df <- data.frame(
  lat = as.numeric(coords["lat", ]),
  lon = as.numeric(coords["lon", ]),
  elevation = as.numeric(elevation), 
  timestamp = as.POSIXct(time,tz="UTC", format=c("%Y-%m-%dT%H:%M:%OS")),
  ActivityName = activity_name,
  ActivityType = activity_type
) 
dfname <- print(substring(gpx_path, 12, 34))
assign(dfname, df, envir = .GlobalEnv)
}

# Iterate over each file and apply your function
for (file_path in file_list) {
  gpx_to_df(file_path)
}

# Combine single track-files to one Dataframe
dflist_Laura <- substring(file_list,12,34)
dflist_Laura
all_tracks_Laura <- do.call(rbind, lapply(dflist_Laura, get))

# Converting the df to sf object
library(sf)
all_tracks_Laura <- st_as_sf(all_tracks_Laura, coords = c("lon", "lat"), crs = 4326)
str(all_tracks_Laura)

# Transforming the crs to CH1903 +LV95 or EPSG:2056 & Timezone
all_tracks_Laura <- st_transform(all_tracks_Laura, 2056)
str(all_tracks_Laura)
# Check Timezone
attr(all_tracks_Laura$timestamp, "tzone")

#  Filtering out old data
all_tracks_Laura <- all_tracks_Laura |> 
  mutate("year" = year(timestamp)) |> #for this use library lubridate
  filter(year == 2024)
```

### Data Exploration

seperate Trajectories (so every activity that was done with a break of two hours gets a new ID)
```{r}

all_tracks_Laura <- all_tracks_Laura %>%
  mutate(timestamp = ymd_hms(timestamp)) %>%
  arrange(timestamp) %>%
  mutate(diff = c(0, diff(timestamp)),
         trajID = cumsum(diff > hours(2)))%>%
  mutate(trajID = trajID + 1)

all_tracks_Laura$diff <- NULL
```

### Map with all trajectories
```{r}
tmap_mode("view")

#reclassify trackID as char
class(all_tracks_Laura$trajID)
as.numeric(all_tracks_Laura$trajID)
all_tracks_Laura$trajID <- as.character(all_tracks_Laura$trajID)

# display all trajectories by trackID 
tm_shape(all_tracks_Laura)+
  tm_dots(col = "trajID", palette = "RdYlGn") 

```
we choose 3 trajectories that we want to analyze. Because we want to classify different movement types within one trajectory, we choose Traj 3 (walking, running, tram), traj 9 (bike and walking) and traj 12 (mostly bike, then stationary/walk)
### choose 3 trajs

?? wofür die visualisierung und bekommst du die hin ohne das trajID von 1-14 angezeigt werdeN???

```{r}
trajIDs <- c(3, 9, 12)

laura_df <- all_tracks_Laura |> 
  filter(trajID %in% trajIDs)
tm_shape(laura_df)+
  tm_dots(col = "trajID", palette = "Paired") 

```
### changing activity Type (irrelevant im moment)
```{r}

all_tracks_Laura <- all_tracks_Laura %>%
  mutate(ActivityType = ifelse(trajID == 2, "car", ActivityType))

all_tracks_Laura <- all_tracks_Laura %>%
  mutate(ActivityType = ifelse(trajID == 3, "mixed", ActivityType))
all_tracks_Laura <- all_tracks_Laura %>%
  mutate(ActivityType = ifelse(trajID == 12, "mixed", ActivityType))


all_tracks_Laura <- all_tracks_Laura %>%
  mutate(ActivityType = ifelse(trajID == 1, "trainride", ActivityType))

all_tracks_Laura <- all_tracks_Laura %>%
  mutate(ActivityType = ifelse(ActivityType == "Canoeing", "trainride", ActivityType))
```

### filter by activity Type & create new DFs with it (irrelevant im moment)

```{r}
 running_Laura <- all_tracks_Laura  |> 
  filter(ActivityType == "running")

bike_Laura <- all_tracks_Laura  |> 
  filter(ActivityType == "cycling")

train <- all_tracks_Laura  |> 
  filter(ActivityType == "trainride")

car_Laura <- all_tracks_Laura  |> 
  filter(trajID == "2") #this one I saw from the movement pattern , or ActivityType == "car"

mixed_Laura <- all_tracks_Laura |> 
  filter(ActivityType=="mixed")
```
### display activities by type(irrelevant im moment)
```{r}

tm_shape(all_tracks_Laura)+
  tm_dots(col = "ActivityType", palette = "Paired") 
```
Here, we can see the 5 different categories, of which one is "mixed", meaning walking, running, tram. 
This one might be interesting to analyze with unsupervised learning algorithm k means. Then we can see if it works for the others as well.  
Therefore, we need to segment by stops, then define the cluster conditions and then cluster the segments accordingly. 


## calculation of parameters
we use the Function distance_by_element in combination with lead() to calculate euclidian distance from point1 to point2. Also, we calculate the time difference from point1 to point2, which normally should be 1 sec, but in case of signal interruption might be bigger. 
Then, we can calculate the speed by distance/time difference and the acceleration per point. 
Then, we apply the moving window filter, with two windows, one of 10 seconds and one of 60 seconds. 
Also, we calculate the elevation distance and sinuosity for more factors to be considered by k means.

FRAGEN: 
-  warum brauchen wir bigger window? with smaller windows/smaller granularity, we can detect finer differences, with a bigger window we can smoothen out BEARBEITEN .
- warum nehmen wir elevation difference und sinuosity mit rein?
 Sinuosity is taken into account as it might differenciate well between public transport and walking or running, as public transport tends to follow straighter lines (on a tram track eg.) We also suppose that this is true for bike vs. tram, but are not sure yet. 
(The elevation difference might also be used as an indicator, together with acceleration, as elevation+slowing down from walking and biking, while it is unlikely that a motorized vehicle slows down as much on a hill. ) $
- elevation over 1 point or 10 points? i took 10 now, but maybe doesnt make sense...

```{r}
 distance_by_element <- function(later, now) {
  as.numeric(
    st_distance(later, now, by_element = TRUE)
  )
 }

laura_df <- laura_df |> 
  group_by(trajID) |>  # Gruppieren nach trajID
  mutate(
    distance = distance_by_element(geometry, lead(geometry, 1)), # distance to pos +1
    time_diff = as.numeric(timestamp - lag(timestamp)),
    speed = distance / time_diff,
    speed_kmh = speed * 3.6, # round ist hier vllt sinnvoll? round(speed*3.6)
    acceleration = (speed - lag(speed)) / time_diff,
    # aus dem package slider, berechnet die gleitende Summe der Werte über 10 Punkte
    avg_speed_10s = slide_dbl(speed_kmh, mean, .before = 5, .after = 5, .complete = TRUE),
    avg_speed_60s = slide_dbl(speed_kmh, mean, .before = 30, .after = 30, .complete = TRUE), 
    # aufpassen! Hier gehen die ersten 60 Datenpunkte verloren!
    max_speed_10s = slide_dbl(speed_kmh, max, .before = 5, .after = 5, .complete = TRUE),
    avg_acc_10s = slide_dbl(acceleration, mean, .before = 5, .after = 5, .complete = TRUE) ,
    avg_acc_60s = slide_dbl(acceleration, mean, .before = 30, .after = 30, .complete = TRUE),
    max_acc_10s = slide_dbl(acceleration, max, .before = 5, .after = 5, .complete = TRUE),
    el_change = (elevation -lag(elevation,  10)), #given in meters/10 datapooints, not sure if correct or makes sense!
    d_direct10 = distance_by_element(lag(geometry,4), lead(geometry,5)),
    d_sinu10 = rollsum(distance, 10,align = "center", fill = NA), # function rollsum does basically the same as slide, was recommended by nils
 #Bei der Verwendung der rollsum-Funktion mit Fenstergröße von 10, werden die Datenpunkte so zentriert, dass die Summe der 5 Punkte vor und der 4 Punkte nach dem aktuellen Punkt berechnet wird.
   d_direct10 = case_when(d_direct10 == 0 ~ d_sinu10,
                                TRUE ~ d_direct10),
 #diese Abänderung von d_direct ist nötig, damit die sinuosity richtig berechnet wird und keine Inf Values herauskommen (wegen Nenner=0)
    sinuosity = d_sinu10/d_direct10 #here, some values are below 1, which does not make sense. Why could this be?? And others are higher than 22, which is also weird, but can be traced back to the stops that are not yet filtered out. (is a step we should still do)
  ) |> 
  ungroup()

which(laura_df$d_direct10 > laura_df$d_sinu10)
which(laura_df$d_direct10 == 0)
summary(laura_df$sinuosity)




#problem if all trajectories are in one table and you calculate distance/speed etc after that the last and first point is always wrongly calculated. Therefore, these outliers are removed:
#laura_df <- laura_df %>%
 # filter(distance <= 1000)

#also doesnt make sense, bc then all other values are wrongly calculated anyway. So - not all in one table but seperately? No, just group and ungroup (is already applied above)

tm_shape(laura_df)+
  tm_dots(col = "el_change", palette = "RdYlGn") 

tm_shape(laura_df)+
  tm_dots(col = "sinuosity", palette = "RdYlGn",
          n =3 ,
          breaks = seq(0, 3, by = 0.1)) 

```

### step mean stuff (irrelevant im moment)
```{r}

# We want a time window of 20 seconds. As we have a point every second, we want to 
mixed_Laura<- mixed_Laura |>
    mutate(
        nMinus2 = distance_by_element(lag(geometry, 10), geometry),  # distance to pos -10 sec
        nMinus1 = distance_by_element(lag(geometry, 5), geometry),  # distance to pos -5 sec
        nPlus1  = distance_by_element(geometry, lead(geometry, 5)), # distance to pos +5 sec
        nPlus2  = distance_by_element(geometry, lead(geometry, 10))  # distance to pos +10 sec 
    )


#but this just calculates the distance by element, not by seconds! In case there was a measurement left out, we do not obtain the correct velocity data, which is the case e.g. in tunnels. Therefore we would need sth like this: (talk to lucia)
#nMinus2 = distance_by_element(lag(geometry, 10), geometry)/difftime(lag(geometry,10)),


# calculate Meanstep
mixed_Laura<- mixed_Laura |>
    rowwise() |>
    mutate(
        stepMean = mean(c(nMinus2, nMinus1, nPlus1, nPlus2))
    ) |>
    ungroup()

tm_shape(mixed_Laura)+
  tm_dots(col = "stepMean", palette = "RdYlGn") 


#we can see that the step mean varies a lot. 


#here, we define the categories according to the step mean 
mixed_Laura  <- mixed_Laura  |>
  mutate(category = case_when(
    stepMean < 20 ~ "walking",
    stepMean >= 20 & stepMean < 40 ~ "running", 
    stepMean >= 40 ~ "tram"
  ))

# somehow this doesnt work??

mixed_Laura  <- mixed_Laura  |>
  mutate(category = case_when(
    speed_kmh < 3.5 ~ "walking",
    speed_kmh >= 3.5 & speed_kmh < 18 ~ "running", 
    speed_kmh >= 20 ~ "tram"
  ))

tm_shape(mixed_Laura)+
  tm_dots(col = "category", palette = "RdYlGn") 


#if we want to exclude the static movements, which in this case leads to the exclusion of half of the trajectory!!! thats actually not waht we want
#mixed_Laura <- mixed_Laura |>
    #mutate(static = stepMean < mean(stepMean, na.rm = TRUE))
#summary(mixed_Laura)

#mixed_Laura_filter <- mixed_Laura |>
    filter(!static)

#tm_shape(mixed_Laura_filter)+
  #tm_dots(col = "stepMean", palette = "RdYlGn") 


```
We cannot use segmentation by static, bc the mean(stepMean) is too high for the stepMean when the person is walking. So, we want to segment by speed change. When the step mean is <20 (meaning 1m/1s=3,6km/h), the category is walking 
when the Step Mean is 20-40  it is running, and everything >40 is tram.  --- wrong thought! We first calculate the speed between two points. 



### Join with spatial data (later?)
```{r}

lines <- read_csv("data/taz.komm_richt_verkehr_l.csv")
lines <- st_as_sf(lines,  wkt="geometry")
lines <- st_set_crs(lines, 2056)

tram_lines <- read_csv("data/2024_vbz_transit/shapes.txt")

lines_select <- lines %>%
  select(objectid, kategorie, teilplan, geometry)

plot(lines_select)

tm_shape(lines_select)+
  tm_lines(col="kategorie") +
tm_shape(mixed_Laura)+
  tm_dots(col="speed_kmh")

#jetzt verbinden wir die beiden Tables, allerdings nicht mit st_intersect, weil es keine gibt, sondern neighbourhood analysis: 

joined <- st_join(mixed_Laura, lines_select, 
                  join = st_is_within_distance, 
                       dist = 15, # Distanz in Metern
                       left = TRUE)# damit auch werte erhalten bleiben, die nicht within distance sind 

# das Anreichern mit Kategorien wird aber später bei k means ein problem, weil er drop NA macht... daher dist=15 anstatt dist=0.5

tm_shape(lines_select)+
  tm_lines(col="kategorie") +
tm_shape(joined)+
  tm_dots(col="speed_kmh")


```

## erster versuch K means
```{r}

#mixed_select <- joined %>%
 # select(distance,speed_kmh, avg_acc_10s, kategorie)
#mixed_select <- st_drop_geometry(mixed_select) # necessary so geometry column goes away, otherwise fviz not work
##mixed_select <- na.omit(mixed_select) #otherwise fehlermeldung

#versuch mit nur numerischen Daten
#plötzlich funktioniert select nicht mehr? This error occurs when you attempt to use the select() function from the dplyr package in R but also have the MASS package loaded. then use dyplr::select()

laura_kmeans <- laura_df |> 
  dplyr::select(distance, time_diff, speed, acceleration, avg_speed_10s, avg_speed_60s, avg_acc_10s, avg_acc_60s, el_change, d_direct10, d_sinu10, sinuosity) |> 
  #filter(speed>0) |> 
  #filter(d_direct10>0)das war von früher, wurde ge debugged

laura_kmeans <- na.omit(laura_kmeans) #otherwise fehlermeldung. Wichtig, erst NA omit, dann  drop geometry, damit später wieder zusammenführbar!
laura_kmeans_no_geom <- st_drop_geometry(laura_kmeans) # necessary so geometry column goes away, otherwise fviz not work

# to find the good amount of k 
plot_k <- fviz_nbclust(laura_kmeans_no_geom, kmeans, method = "wss") #takes 3 mins to calculate, gives 5 clusters
#interesting: the "elbow"/knick, which indicates the appropriate k value, changes when we add sinuosity parameter from 5 to 4. So we try k means with both k values!

#make this example reproducible, to create random numbers that can be reproduced (was given in code, dont know why)
set.seed(1)

#apply k means 
km25 <- kmeans(laura_kmeans_no_geom, centers = 5, nstart = 25)

# nstart: The number of initial configurations. Because it’s possible that different initial starting clusters can lead to different results, it’s recommended to use several different initial configurations. The k-means algorithm will find the initial configurations that lead to the smallest within-cluster variation. 

plot_cluster25_5 <- fviz_cluster(km25, data = laura_kmeans_no_geom)

laura_kmeans<- cbind(laura_kmeans, cluster25_5 = km25$cluster) #achtung, habe hier das kmeans MIT gemoetrie verwendet, und bin mir nicht sicher, ob es die Cluster dann in der richtigen Reihenfolge anordnet!

tm_shape(laura_kmeans)+
  tm_dots(col = "cluster25_5", palette = "RdYlGn") 
#das cluster klappt noch ziemlich schlecht, es clustert zB die standing points gleich wie das laufen. Und Strassenbahn und Fahrrad sind ebenfalls gleich

```

Changing the nstart value to 50
```{r}

km50 <- kmeans(laura_kmeans_no_geom, centers = 5, nstart = 50)
km50
```

Changing the cluster number to 4?
```{r}

km25_4 <- kmeans(laura_kmeans_no_geom, centers = 4, nstart = 25)
km25_4
```

k means kann nicht gut mit nicht numerischen (also zB kategorie) Darten umgehen.

K-Modes: Ein Algorithmus, der speziell für kategorische Daten entwickelt wurde. Er verwendet die Hamming-Distanz oder andere Distanzmetriken für kategorische Daten.
Diesen könnten wir verwenden. 

## kmeans with modes

```{r}
install.packages("klaR")
library(klaR)

mixed_select$kategorie <- as.factor(mixed_select$kategorie)
set.seed(123) # Für Reproduzierbarkeit
km_res <- kmodes(mixed_select, modes = 3) # Anzahl der Cluster festlegen

cluster_assignments <- km_res$cluster

ggplot(df, aes(x = distance, y = speed_kmh, color = factor(cluster_assignments))) +
  geom_point() +
  facet_wrap(~kategorie)
```


## Sinuosity var Nils

```{r}
 distance_by_element <- function(later, now) {
  as.numeric(
    st_distance(later, now, by_element = TRUE)
  )
 }

library(zoo)


mixed_Laura |> 
  mutate(
    direct = distance_by_element(lag(geometry,10), lead(geometry,10)),
    sinu = rollsum(distance, 10,align = "center", fill = NA)
  ) |> View()

?rollsum
  
```



## calculate sinuosity
```{r}
install.packages("trajr")
library(trajr)

coords <- st_coordinates(mixed_Laura)

# Add x and y columns to the sf object
mixed_Laura$x <- coords[,1]
mixed_Laura$y <- coords[,2]

# doesnt work. 

trj <- TrajsBuild(data = "mixed_Laura", x = "x", y = "y", time = "timestamp", geometry = "geometry")

sinuosity <- TrajSinuosity2(trj)

```


# Old Stuff (not to use)

### Understanding GPX 
https://www.appsilon.com/post/r-gpx-files
GPX is a very common map data format used to store GPS data of routes. GPX is basically XML, therefore, we need to install the XML package. 

- GPX is just a fancier version of xml, so we can recycle sml tools
We will load the data of one GPX track as a html file, but the loaded gpx data looks pretty messy, It is thus necessary to tidy it by identifying key structures: 
- trkpt element = contains latitude and longitude information for every point
- ele tag = contains the elevation.
- time = contains UTC-8 Timeinformation

The html looks like this: 
</trkpt><trkpt lat="47.2176510" lon="8.6811000"> <ele>511.9<
/ele><time>2024-03-25T16:03:41Z<
/time><extensions><trackpointextension><cad>0</cad></trackpointextension></extensions>



```{r}
#the coord info is in gpx file for every activity. 
coords <- xpathSApply(doc = gpx_1, path = "//trkpt", fun = xmlAttrs) # question: if we use the same from gpx1 for all, is that a problem?
elevation <- xpathSApply(doc = gpx_1, path = "//trkpt/ele", fun = xmlValue) # same question

df <- data.frame(
  lat = as.numeric(coords["lat", ]),
  lon = as.numeric(coords["lon", ]),
  elevation = as.numeric(elevation)
)

head(df, 10)
tail(df, 10)

plot(x = df$lon, y = df$lat, type = "l", col = "black", lwd = 3,
     xlab = "Longitude", ylab = "Latitude")
```
###CHAT GTP idea: 
make a df that has all the lat lon and elevation infos of all the trips, and give them a column with a unique ID per trip. 
then this huge df can be set into one map, structured by ID. 
therefore, I would need to add a column that has the ID 
```{r}
#CHAT GTP proposes this: 
gpx_files <- list.files("data/Laura_Strava/activities", pattern = "\\.gpx$", full.names = TRUE)

head(gpx_files)

process_gpx <- function(file, id) {
  doc <- xmlTreeParse(file, useInternalNodes = TRUE)
  coords <- xpathApply(doc, "//trkpt", function(x) as.numeric(xmlAttrs(x)[c("lat", "lon")]))
  elevation <- xpathSApply(doc, "//trkpt/ele", xmlValue)
  df <- data.frame(
    lat = coords[, 1],
    lon = coords[, 2],
    elevation = as.numeric(elevation),
    object_id = id
  )
  return(df)
}

all_data <- do.call(rbind, lapply(seq_along(gpx_files), function(i) {
  process_gpx(gpx_files[i], i)
}))

head(all_data)

```


```{r}
library(XML)

# Create a list of all GPX file paths
gpx_files <- list.files("data/Laura_Strava/activities", pattern = "\\.gpx$", full.names = TRUE)

# Function to process a single GPX file
process_gpx <- function(file, id) {
  doc <- xmlTreeParse(file, useInternalNodes = TRUE)
  
  coords <- xpathApply(doc, "//trkpt", function(node) {
    lat <- as.numeric(xmlValue(node["lat"]))
    lon <- as.numeric(xmlValue(node["lon"]))
    return(c(lat, lon))
  })
  
  elevation <- xpathSApply(doc, "//trkpt/ele", xmlValue)
  
  df <- data.frame(
    id = id,
    lat = unlist(coords[1,]),
    lon = unlist(coords[2,]),
    elevation = as.numeric(elevation)
  )
  
  return(df)
}

# Process all GPX files and combine into a single dataframe
all_data <- do.call(rbind, lapply(seq_along(gpx_files), function(i) {
  process_gpx(gpx_files[i], i)
}))

```


### loading several files into one Map
https://stackoverflow.com/questions/54726758/merging-multiple-gpx-files-into-a-single-gpx-file-with-multiple-tracks --- didnt work, as old plotKML used and bc of other stuff. 

```{r}
gpx_files <- c("data/Laura_Strava/activities/11091356418.gpx","data/Laura_Strava/activities/11103101530.gpx", "data/Laura_Strava/activities/11116616348.gpx", "data/Laura_Strava/activities/11188517987.gpx", "data/Laura_Strava/activities/11203431760.gpx", "data/Laura_Strava/activities/11209427592.gpx", "data/Laura_Strava/activities/11239313364.gpx")


```

```{r}
```




## colouring etc
```{r}
library(leaflet)

leaflet() %>%
  addTiles() %>%
  addPolylines(data = df, lat = ~lat, lng = ~lon, color = "#000000", opacity = 0.8, weight = 3)


get_color <- function(elevation) {
  if (elevation < 500) {
    return("green")
  }
  if (elevation < 1000) {
    return("yellow")
  }
  if (elevation < 1500) {
    return("orange")
  }
  return("red")
}




# New dataset with the new variable for color
df_color <- df %>%
  rowwise() %>%
  mutate(color = get_color(elevation))

df_color$last_color <- dplyr::lag(df_color$color)

# Map
map <- leaflet() %>% addTiles()
for (color in levels(as.factor(df_color$color))) {
  map <- addPolylines(map, lat = ~lat, lng = ~lon, data = df_color[df_color$color == color | df_color$last_color == color, ], color = ~color)
}
map
```
