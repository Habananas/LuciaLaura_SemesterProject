---
title: "Strava_File"
format: html
---

# STRAVA PROJECT 

### Libraries and Files
```{r}
# install.packages("XML")
# install.packages("gitcreds")
library("readr")
library(sf) #simple features 
library(ggplot2)
library(dplyr)
library("gitcreds")
library(XML) #to read the XML data of gpx files
library(leaflet) #to show in a map
library(lubridate) # time
library(knitr) #To “prove” that script runs on your machine from top to bottom
library(slider) # for "sliding" over the datapoints (similar to leadlag or roll)
library(factoextra)#kmeans
library(cluster)#kmeans
```

### Understanding GPX 
https://www.appsilon.com/post/r-gpx-files
GPX is a very common map data format used to store GPS data of routes. GPX is basically XML, therefore, we need to install the XML package. 

- GPX is just a fancier version of xml, so we can recycle sml tools
We will load the data of one GPX track as a html file, but the loaded gpx data looks pretty messy, It is thus necessary to tidy it by identifying key structures: 
- trkpt element = contains latitude and longitude information for every point
- ele tag = contains the elevation.
- time = contains UTC-8 Timeinformation

The html looks like this: 
</trkpt><trkpt lat="47.2176510" lon="8.6811000"> <ele>511.9<
/ele><time>2024-03-25T16:03:41Z<
/time><extensions><trackpointextension><cad>0</cad></trackpointextension></extensions>


### Data Loading and Organisation
```{r}
laura_act <- read.csv("data/activities_Laura.csv")

# Get a list of files in the folder
folder_path <- "data/activities_Laura/"
file_list <- list.files(folder_path, full.names = TRUE)

#Create a function that assigns coordinates, elevation, time and activity name out of the gpx file, then apply this function to all of the gpx files:
gpx_to_df <- function(gpx_path) {
  gpx_parsed <- htmlTreeParse(file = gpx_path, useInternalNodes = TRUE)
  # read out elements of the html file to vecotrs
coords <- xpathSApply(doc = gpx_parsed, path = "//trkpt", fun = xmlAttrs)
elevation <- xpathSApply(doc = gpx_parsed, path = "//trkpt/ele", fun = xmlValue)
time <- xpathSApply(doc = gpx_parsed, path = "//time", fun = xmlValue)
activity_name <- xpathSApply(doc = gpx_parsed, path = "//name", fun = xmlValue)
activity_type <- xpathSApply(doc = gpx_parsed, path = "//type", fun = xmlValue)
# remove first value of time, as it stems from the metadata and matches the second value (i.e. first timestamp of trackpoint)
time <- time[-1]
# convert vectors to a data frame
df <- data.frame(
  lat = as.numeric(coords["lat", ]),
  lon = as.numeric(coords["lon", ]),
  elevation = as.numeric(elevation), 
  timestamp = as.POSIXct(time,tz="UTC", format=c("%Y-%m-%dT%H:%M:%OS")),
  ActivityName = activity_name,
  ActivityType = activity_type
) 
dfname <- print(substring(gpx_path, 12, 34))
assign(dfname, df, envir = .GlobalEnv)
}

# Iterate over each file and apply your function
for (file_path in file_list) {
  gpx_to_df(file_path)
}

# Combine single track-files to one Dataframe
dflist_Laura <- substring(file_list,12,34)
dflist_Laura
all_tracks_Laura <- do.call(rbind, lapply(dflist_Laura, get))

# Converting the df to sf object
library(sf)
all_tracks_Laura <- st_as_sf(all_tracks_Laura, coords = c("lon", "lat"), crs = 4326)
str(all_tracks_Laura)

# Transforming the crs to CH1903 +LV95 or EPSG:2056 & Timezone
all_tracks_Laura <- st_transform(all_tracks_Laura, 2056)
str(all_tracks_Laura)
# Check Timezone
attr(all_tracks_Laura$timestamp, "tzone")

#  Filtering out old data
all_tracks_Laura <- all_tracks_Laura |> 
  mutate("year" = year(timestamp)) |> #for this use library lubridate
  filter(year == 2024)
```

### Data Exploration

(LAURA)
seperate Trajectories (so every activity that was done with a break of two hours gets a new ID)
```{r}

all_tracks_Laura <- all_tracks_Laura %>%
  mutate(timestamp = ymd_hms(timestamp)) %>%
  arrange(timestamp) %>%
  mutate(diff = c(0, diff(timestamp)),
         trajID = cumsum(diff > hours(2)))%>%
  mutate(trajID = trajID + 1)

all_tracks_Laura$diff <- NULL
```

### Making a map of the data
```{r}
library(tmap)

tmap_mode("view")

#reclassify trackID as char:
class(all_tracks_Laura$trajID)
as.numeric(all_tracks_Laura$trajID)
all_tracks_Laura$trajID <- as.character(all_tracks_Laura$trajID)

# display all trajectories by trackID 
tm_shape(all_tracks_Laura)+
  tm_dots(col = "trajID", palette = "RdYlGn") 

```
we choose 3 trajectories that we want to analyze. Because we want to classify different movement types within one trajectory, we choose Traj 3 (walking, running, tram), traj 9 (bike and walking) and traj 12 (mostly bike, then stationary/walk)
### choose 3 trajs
```{r}
trajIDs <- c(3, 9, 12)

laura_df <- all_tracks_Laura |> 
  filter(trajID %in% trajIDs)
tm_shape(laura_df)+
  tm_dots(col = "trajID", palette = "Paired") 

```
### changing activity Type (irrelevant im moment)
```{r}

all_tracks_Laura <- all_tracks_Laura %>%
  mutate(ActivityType = ifelse(trajID == 2, "car", ActivityType))

all_tracks_Laura <- all_tracks_Laura %>%
  mutate(ActivityType = ifelse(trajID == 3, "mixed", ActivityType))
all_tracks_Laura <- all_tracks_Laura %>%
  mutate(ActivityType = ifelse(trajID == 12, "mixed", ActivityType))


all_tracks_Laura <- all_tracks_Laura %>%
  mutate(ActivityType = ifelse(trajID == 1, "trainride", ActivityType))

all_tracks_Laura <- all_tracks_Laura %>%
  mutate(ActivityType = ifelse(ActivityType == "Canoeing", "trainride", ActivityType))
```

### filter by activity Type & create new DFs with it (irrelevant im moment)

```{r}
 running_Laura <- all_tracks_Laura  |> 
  filter(ActivityType == "running")

bike_Laura <- all_tracks_Laura  |> 
  filter(ActivityType == "cycling")

train <- all_tracks_Laura  |> 
  filter(ActivityType == "trainride")

car_Laura <- all_tracks_Laura  |> 
  filter(trajID == "2") #this one I saw from the movement pattern , or ActivityType == "car"

mixed_Laura <- all_tracks_Laura |> 
  filter(ActivityType=="mixed")
```
### display activities by type(irrelevant im moment)
```{r}

tm_shape(all_tracks_Laura)+
  tm_dots(col = "ActivityType", palette = "Paired") 
```
Here, we can see the 5 different categories, of which one is "mixed", meaning walking, running, tram. 
This one might be interesting to analyze with unsupervised learning algorithm k means. Then we can see if it works for the others as well.  
Therefore, we need to segment by stops, then define the cluster conditions and then cluster the segments accordingly. 


## calculation of parameters
we use the Function distance_by_element in combination with lead() to calculate euclidian distance from point1 to point2. Also, we calculate the time difference from poitn1 to point2, which normally should be 1 sec, but in case of signal interruption might be bigger. 
Then, we can calculate the speed by distance/time difference, the acceleration per point. 
Then, we apply the moving window filter, with two windows, one of 10 seconds and one of 60 seconds. (BEGRUENDUNG: mit einem kleineren Window können wir die 


the means and maxs of it
```{r}
 distance_by_element <- function(later, now) {
  as.numeric(
    st_distance(later, now, by_element = TRUE)
  )
 }

#install.packages("slider")
library(slider)

laura_df <- laura_df |> 
  group_by(trajID) |>  # Gruppieren nach trajID
  mutate(
     distance = distance_by_element(geometry, lead(geometry, 1)), # distance to pos +1
    time_diff = c(NA, difftime(timestamp[-1], timestamp[-length(timestamp)], units = "secs")),
    speed = distance / time_diff,
    speed_kmh = speed * 3.6, # round ist hier vllt sinnvoll? round(speed*3.6)
    acceleration = (speed - lag(speed)) / time_diff,
    # aus dem package slider, berechnet die gleitende Summe der Werte über 10 Punkte
    avg_speed_10s = slide_dbl(speed_kmh, sum, .before = 5, .after = 5, .complete = TRUE) /
      slide_dbl(time_diff, sum, .before = 5, .after = 5, .complete = TRUE),
    avg_speed_60s = slide_dbl(speed_kmh, sum, .before = 30, .after = 30, .complete = TRUE) /
      slide_dbl(time_diff, sum, .before = 30, .after = 30, .complete = TRUE), 
    # aufpassen! Hier gehen die ersten 60 Datenpunkte verloren!
        max_speed_10s = slide_dbl(speed_kmh, max, .before = 5, .after = 5, .complete = TRUE),
    avg_acc_10s = slide_dbl(acceleration, sum, .before = 5, .after = 5, .complete = TRUE) /
      slide_dbl(time_diff, sum, .before = 5, .after = 5, .complete = TRUE),
    avg_acc_60s = slide_dbl(acceleration, sum, .before = 30, .after = 30, .complete = TRUE) /
      slide_dbl(time_diff, sum, .before = 30, .after = 30, .complete = TRUE),
     max_acc_10s = slide_dbl(acceleration, max, .before = 5, .after = 5, .complete = TRUE),
    el_change = (elevation -lag(elevation,  10)) #given in meters/10 datapooints. Not sure if correct?
    
  ) |> 
  ungroup()


#problem if all trajectories are in one table and you calculate distance/speed etc after that the last and first point is always wrongly calculated. Therefore, these outliers are removed:
#laura_df <- laura_df %>%
 # filter(distance <= 1000)

#also doesnt make sense, bc then all other values are wrongly calculated anyway. So - not all in one table but seperately? No, just group and ungroup (is already applied above)

tm_shape(laura_df)+
  tm_dots(col = "el_change", palette = "RdYlGn") 


```

### step mean stuff
```{r}

# We want a time window of 20 seconds. As we have a point every second, we want to 
mixed_Laura<- mixed_Laura |>
    mutate(
        nMinus2 = distance_by_element(lag(geometry, 10), geometry),  # distance to pos -10 sec
        nMinus1 = distance_by_element(lag(geometry, 5), geometry),  # distance to pos -5 sec
        nPlus1  = distance_by_element(geometry, lead(geometry, 5)), # distance to pos +5 sec
        nPlus2  = distance_by_element(geometry, lead(geometry, 10))  # distance to pos +10 sec 
    )


#but this just calculates the distance by element, not by seconds! In case there was a measurement left out, we do not obtain the correct velocity data, which is the case e.g. in tunnels. Therefore we would need sth like this: (talk to lucia)
#nMinus2 = distance_by_element(lag(geometry, 10), geometry)/difftime(lag(geometry,10)),


# calculate Meanstep
mixed_Laura<- mixed_Laura |>
    rowwise() |>
    mutate(
        stepMean = mean(c(nMinus2, nMinus1, nPlus1, nPlus2))
    ) |>
    ungroup()

tm_shape(mixed_Laura)+
  tm_dots(col = "stepMean", palette = "RdYlGn") 


#we can see that the step mean varies a lot. 


#here, we define the categories according to the step mean 
mixed_Laura  <- mixed_Laura  |>
  mutate(category = case_when(
    stepMean < 20 ~ "walking",
    stepMean >= 20 & stepMean < 40 ~ "running", 
    stepMean >= 40 ~ "tram"
  ))

# somehow this doesnt work??

mixed_Laura  <- mixed_Laura  |>
  mutate(category = case_when(
    speed_kmh < 3.5 ~ "walking",
    speed_kmh >= 3.5 & speed_kmh < 18 ~ "running", 
    speed_kmh >= 20 ~ "tram"
  ))

tm_shape(mixed_Laura)+
  tm_dots(col = "category", palette = "RdYlGn") 


#if we want to exclude the static movements, which in this case leads to the exclusion of half of the trajectory!!! thats actually not waht we want
#mixed_Laura <- mixed_Laura |>
    #mutate(static = stepMean < mean(stepMean, na.rm = TRUE))
#summary(mixed_Laura)

#mixed_Laura_filter <- mixed_Laura |>
    filter(!static)

#tm_shape(mixed_Laura_filter)+
  #tm_dots(col = "stepMean", palette = "RdYlGn") 


```
We cannot use segmentation by static, bc the mean(stepMean) is too high for the stepMean when the person is walking. So, we want to segment by speed change. When the step mean is <20 (meaning 1m/1s=3,6km/h), the category is walking 
when the Step Mean is 20-40  it is running, and everything >40 is tram.  --- wrong thought! We first calculate the speed between two points. 



### Join with spatial data
```{r}

lines <- read_csv("data/taz.komm_richt_verkehr_l.csv")
lines <- st_as_sf(lines,  wkt="geometry")
lines <- st_set_crs(lines, 2056)

tram_lines <- read_csv("data/2024_vbz_transit/shapes.txt")

lines_select <- lines %>%
  select(objectid, kategorie, teilplan, geometry)

plot(lines_select)

tm_shape(lines_select)+
  tm_lines(col="kategorie") +
tm_shape(mixed_Laura)+
  tm_dots(col="speed_kmh")

#jetzt verbinden wir die beiden Tables, allerdings nicht mit st_intersect, weil es keine gibt, sondern neighbourhood analysis: 

joined <- st_join(mixed_Laura, lines_select, 
                  join = st_is_within_distance, 
                       dist = 15, # Distanz in Metern
                       left = TRUE)# damit auch werte erhalten bleiben, die nicht within distance sind 

# das Anreichern mit Kategorien wird aber später bei k means ein problem, weil er drop NA macht... daher dist=15 anstatt dist=0.5

tm_shape(lines_select)+
  tm_lines(col="kategorie") +
tm_shape(joined)+
  tm_dots(col="speed_kmh")


```

## erster versuch K means
```{r}

#install.packages("factoextra")
#install.packages("cluster")
library(factoextra)
library(cluster)

#mixed_select <- joined %>%
 # select(distance,speed_kmh, avg_acc_10s, kategorie)
#mixed_select <- st_drop_geometry(mixed_select) # necessary so geometry column goes away, otherwise fviz not work
##mixed_select <- na.omit(mixed_select) #otherwise fehlermeldung

#versuch mit nur numerischen Daten
#plötzlich funktioniert select nicht mehr? This error occurs when you attempt to use the select() function from the dplyr package in R but also have the MASS package loaded. then use dyplr::select()

laura_kmeans <- laura_df |> 
  dplyr::select(distance, time_diff, speed, acceleration, avg_speed_10s, avg_speed_60s, avg_acc_10s, avg_acc_60s, el_change)
laura_kmeans <- na.omit(laura_kmeans) #otherwise fehlermeldung. Wichtig, erst NA omit, dann  drop geometry, damit später wieder zusammenführbar!
laura_kmeans_no_geom <- st_drop_geometry(laura_kmeans) # necessary so geometry column goes away, otherwise fviz not work

# to find the good amount of k 
fviz_nbclust(laura_kmeans_no_geom, kmeans, method = "wss") #takes 3 mins to calculate, gives 5 clusters

#make this example reproducible
set.seed(1)
#apply k means 
km <- kmeans(laura_kmeans_no_geom, centers = 5, nstart = 25)
km
# nstart: The number of initial configurations. Because it’s possible that different initial starting clusters can lead to different results, it’s recommended to use several different initial configurations. The k-means algorithm will find the initial configurations that lead to the smallest within-cluster variation. 

fviz_cluster(km, data = laura_kmeans_no_geom)

laura_kmeans<- cbind(laura_kmeans, cluster = km$cluster) #achtung, habe hier das kmeans MIT gemoetrie verwendet, und bin mir nicht sicher, ob es die Cluster dann in der richtigen Reihenfolge anordnet!

tm_shape(laura_kmeans)+
  tm_dots(col = "cluster", palette = "RdYlGn") 
#das cluster klappt noch ziemlich schlecht, es clustert zB die standing points gleich wie das laufen. Und Strassenbahn und Fahrrad sind ebenfalls gleich

```
k means kann nicht gut mit nicht numerischen (also zB kategorie) Darten umgehen.

K-Modes: Ein Algorithmus, der speziell für kategorische Daten entwickelt wurde. Er verwendet die Hamming-Distanz oder andere Distanzmetriken für kategorische Daten.
Diesen könnten wir verwenden. 

## kmeans with modes

```{r}
install.packages("klaR")
library(klaR)

mixed_select$kategorie <- as.factor(mixed_select$kategorie)
set.seed(123) # Für Reproduzierbarkeit
km_res <- kmodes(mixed_select, modes = 3) # Anzahl der Cluster festlegen

cluster_assignments <- km_res$cluster

ggplot(df, aes(x = distance, y = speed_kmh, color = factor(cluster_assignments))) +
  geom_point() +
  facet_wrap(~kategorie)
```


## Sinuosity var Nils

```{r}
 distance_by_element <- function(later, now) {
  as.numeric(
    st_distance(later, now, by_element = TRUE)
  )
 }

library(zoo)


mixed_Laura |> 
  mutate(
    direct = distance_by_element(lag(geometry,10), lead(geometry,10)),
    sinu = rollsum(distance, 10,align = "center", fill = NA)
  ) |> View()

?rollsum
  
```



## calculate sinuosity
```{r}
install.packages("trajr")
library(trajr)

coords <- st_coordinates(mixed_Laura)

# Add x and y columns to the sf object
mixed_Laura$x <- coords[,1]
mixed_Laura$y <- coords[,2]

# doesnt work. 

trj <- TrajsBuild(data = "mixed_Laura", x = "x", y = "y", time = "timestamp", geometry = "geometry")

sinuosity <- TrajSinuosity2(trj)

```


# Old Stuff (not to use)

```{r}
#the coord info is in gpx file for every activity. 
coords <- xpathSApply(doc = gpx_1, path = "//trkpt", fun = xmlAttrs) # question: if we use the same from gpx1 for all, is that a problem?
elevation <- xpathSApply(doc = gpx_1, path = "//trkpt/ele", fun = xmlValue) # same question

df <- data.frame(
  lat = as.numeric(coords["lat", ]),
  lon = as.numeric(coords["lon", ]),
  elevation = as.numeric(elevation)
)

head(df, 10)
tail(df, 10)

plot(x = df$lon, y = df$lat, type = "l", col = "black", lwd = 3,
     xlab = "Longitude", ylab = "Latitude")
```
###CHAT GTP idea: 
make a df that has all the lat lon and elevation infos of all the trips, and give them a column with a unique ID per trip. 
then this huge df can be set into one map, structured by ID. 
therefore, I would need to add a column that has the ID 
```{r}
#CHAT GTP proposes this: 
gpx_files <- list.files("data/Laura_Strava/activities", pattern = "\\.gpx$", full.names = TRUE)

head(gpx_files)

process_gpx <- function(file, id) {
  doc <- xmlTreeParse(file, useInternalNodes = TRUE)
  coords <- xpathApply(doc, "//trkpt", function(x) as.numeric(xmlAttrs(x)[c("lat", "lon")]))
  elevation <- xpathSApply(doc, "//trkpt/ele", xmlValue)
  df <- data.frame(
    lat = coords[, 1],
    lon = coords[, 2],
    elevation = as.numeric(elevation),
    object_id = id
  )
  return(df)
}

all_data <- do.call(rbind, lapply(seq_along(gpx_files), function(i) {
  process_gpx(gpx_files[i], i)
}))

head(all_data)

```


```{r}
library(XML)

# Create a list of all GPX file paths
gpx_files <- list.files("data/Laura_Strava/activities", pattern = "\\.gpx$", full.names = TRUE)

# Function to process a single GPX file
process_gpx <- function(file, id) {
  doc <- xmlTreeParse(file, useInternalNodes = TRUE)
  
  coords <- xpathApply(doc, "//trkpt", function(node) {
    lat <- as.numeric(xmlValue(node["lat"]))
    lon <- as.numeric(xmlValue(node["lon"]))
    return(c(lat, lon))
  })
  
  elevation <- xpathSApply(doc, "//trkpt/ele", xmlValue)
  
  df <- data.frame(
    id = id,
    lat = unlist(coords[1,]),
    lon = unlist(coords[2,]),
    elevation = as.numeric(elevation)
  )
  
  return(df)
}

# Process all GPX files and combine into a single dataframe
all_data <- do.call(rbind, lapply(seq_along(gpx_files), function(i) {
  process_gpx(gpx_files[i], i)
}))

```


### loading several files into one Map
https://stackoverflow.com/questions/54726758/merging-multiple-gpx-files-into-a-single-gpx-file-with-multiple-tracks --- didnt work, as old plotKML used and bc of other stuff. 

```{r}
gpx_files <- c("data/Laura_Strava/activities/11091356418.gpx","data/Laura_Strava/activities/11103101530.gpx", "data/Laura_Strava/activities/11116616348.gpx", "data/Laura_Strava/activities/11188517987.gpx", "data/Laura_Strava/activities/11203431760.gpx", "data/Laura_Strava/activities/11209427592.gpx", "data/Laura_Strava/activities/11239313364.gpx")


```

```{r}
```




## colouring etc
```{r}
library(leaflet)

leaflet() %>%
  addTiles() %>%
  addPolylines(data = df, lat = ~lat, lng = ~lon, color = "#000000", opacity = 0.8, weight = 3)


get_color <- function(elevation) {
  if (elevation < 500) {
    return("green")
  }
  if (elevation < 1000) {
    return("yellow")
  }
  if (elevation < 1500) {
    return("orange")
  }
  return("red")
}




# New dataset with the new variable for color
df_color <- df %>%
  rowwise() %>%
  mutate(color = get_color(elevation))

df_color$last_color <- dplyr::lag(df_color$color)

# Map
map <- leaflet() %>% addTiles()
for (color in levels(as.factor(df_color$color))) {
  map <- addPolylines(map, lat = ~lat, lng = ~lon, data = df_color[df_color$color == color | df_color$last_color == color, ], color = ~color)
}
map
```
